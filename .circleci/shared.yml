# Adopted from https://github.com/Pocket/pocket-monorepo/tree/main/.circleci
version: 2.1

orbs:
  aws-cli: circleci/aws-cli@2.0.6
  aws-ecr: circleci/aws-ecr@7.3.0
  aws-code-deploy: circleci/aws-code-deploy@3.0.0
  jq: circleci/jq@2.2.0

# This is an enum that is used within all our jobs and our exit early job.
# As a new "service/deployment" is added you should add to the enum.
# Then each job you pass a "for" to, so that we can determine if this job is for this "commit"
repo_for_enum: &repo_for_enum
  for:
    description: which repo this job is relevant for
    type: enum
    enum:
      - prospect_api
      - curated_corpus_api
      - collection_api
      - corpus_scheduler_lambda
      - prospect_translation_lambda

resource_class_enum: &resource_class_enum
  resource-class:
    description: The self hosted runnner to run on
    type: enum
    enum:
      - pocket/default-dev
      - pocket/default-prod


parameters:
  prospect_api:
    type: boolean
    default: false
  curated_corpus_api:
    type: boolean
    default: false
  collection_api:
    type: boolean
    default: false
  corpus_scheduler_lambda:
    type: boolean
    default: false
  prospect_translation_lambda:
    type: boolean
    default: false

commands:
  # Refrenced from https://github.com/kelvintaywl-cci/dynamic-config-showcase/blob/main/.circleci/next.yml
  exit-early-if-irrelevant:
    parameters:
      <<: *repo_for_enum
    steps:
      - run:
          name: stop early unless relevant
          command: |
            # looks up the relevant pipeline parameter via the env var
            export RELEVANT=$(eval echo "\$<< parameters.for >>")

            # NOTE: env var values are strings (not boolean)
            if [ "${RELEVANT}" = "1" ]; then
              echo "continuing, since job is for << parameters.for >>"
            else
              echo "stopping early!"
              circleci-agent step halt
            fi
          environment:
            prospect_api: << pipeline.parameters.prospect_api >>
            curated_corpus_api: << pipeline.parameters.curated_corpus_api >>
            collection_api: << pipeline.parameters.collection_api >>
            corpus_scheduler_lambda: << pipeline.parameters.corpus_scheduler_lambda >>
            prospect_translation_lambda: << pipeline.parameters.prospect_translation_lambda >>
  install_pnpm:
    parameters:
      scope:
        description: The pnpm recognized name of the service. Used to scope the packages installed to only that service.
        type: string
        default: ""
    steps:
      - run:
          name: Install pnpm package manager
          command: |
            sudo corepack enable
            sudo corepack prepare pnpm@8.15.3 --activate
            pnpm config set store-dir .pnpm-store
      # if a scope was passed to this command (e.g. "curated-corpus-api"), then only the packages in that scope
      # will be installed. this is beneficial to CI build/run times.
      - when:
          condition: <<parameters.scope>>
          steps:
            - restore_cache:
                name: Restore pnpm Package Cache
                keys:
                  - pnpmPackages-<< parameters.scope >>-{{ checksum "pnpm-lock.yaml" }}
            - run:
                name: Install Dependencies
                # Need to set peer-deps to false for pnpmv8 https://github.com/pnpm/pnpm/issues/6300
                command: |
                  pnpm install --filter=<< parameters.scope >>... --config.dedupe-peer-dependents=false
            - save_cache:
                name: Save pnpm Package Cache
                key: pnpmPackages-<< parameters.scope >>-{{ checksum "pnpm-lock.yaml" }}
                paths:
                  - .pnpm-store
      # if no scope was passed, we install all dependencies
      - unless:
          condition: <<parameters.scope>>
          steps:
            - restore_cache:
                name: Restore pnpm Package Cache
                keys:
                  - pnpm-packages-{{ checksum "pnpm-lock.yaml" }}
            - run:
                name: Install Dependencies
                command: |
                  pnpm install
            - save_cache:
                name: Save pnpm Package Cache
                key: pnpm-packages-{{ checksum "pnpm-lock.yaml" }}
                paths:
                  - .pnpm-store
  generate_prisma_client:
    steps:
      - run:
          name: Generate prisma client
          command: pnpm run db:generate-prisma-client
  install_infrastructure_pnpm:
    steps:
      - run:
          name: Install and setup node
          command: |
            . /home/circleci/.codebuild_shims_wrapper.sh
            nvm install
            nvm use
            npm install -g pnpm@8.15.3
            pnpm install
  install_codebuild_secrets:
    steps:
      - run:
          name: Setup our secrets from AWS Secret Manager
          command: |
            . /home/circleci/.codebuild_shims_wrapper.sh
            echo 'export SECRET_VALUE="$(aws secretsmanager get-secret-value --secret-id CodeBuild/Default --query SecretString --output text)"' >> "$BASH_ENV"
            echo 'export TERRAFORM_TOKEN="$(echo $SECRET_VALUE | jq -r '.terraform_token')"' >> "$BASH_ENV"
            echo 'export PAGERDUTY_TOKEN="$(echo $SECRET_VALUE | jq -r '.mozilla_pagerduty_token')"' >> "$BASH_ENV"
      - run:
          name: Save off terraform token
          command: |
            echo Setting Up Terraform Token
            rc="credentials \"app.terraform.io\" { "
            rc="${rc} token=\"$TERRAFORM_TOKEN\" "
            rc="${rc}}"
            echo "$rc" > ~/.terraformrc
  setup_github_bot:
    steps:
      - run:
          name: Get Github Bot Token
          command: |
            app_id=$GITHUB_APP_ID
            pem="$(echo "$GITHUB_APP_PRIVATE_KEY" | base64 -d)"
            installation_id=$GITHUB_INSTALLATION_APP_ID

            now=$(date +%s)
            iat=$((${now} - 60)) # Issues 60 seconds in the past
            exp=$((${now} + 600)) # Expires 10 minutes in the future

            b64enc() { openssl base64 | tr -d '=' | tr '/+' '_-' | tr -d '\n'; }

            header_json='{
                "typ":"JWT",
                "alg":"RS256"
            }'
            # Header encode
            header=$( echo -n "${header_json}" | b64enc )

            payload_json='{
                "iat":'"${iat}"',
                "exp":'"${exp}"',
                "iss":'"${app_id}"'
            }'
            # Payload encode
            payload=$( echo -n "${payload_json}" | b64enc )

            # Signature
            header_payload="${header}"."${payload}"
            signature=$( 
                openssl dgst -sha256 -sign <(echo -n "${pem}") \
                <(echo -n "${header_payload}") | b64enc 
            )

            # Create JWT
            JWT="${header_payload}"."${signature}"

            # Make a POST request to GitHub API to get the installation token
            response=$(curl -s -X POST \
              -H "Accept: application/vnd.github.v3+json" \
              -H "Authorization: Bearer $JWT" \
              -d "{}" \
              "https://api.github.com/app/installations/$installation_id/access_tokens")

            # Extract the token from the response
            token=$(echo "$response" | jq -r '.token')
            echo "export GITHUB_TOKEN=$token" >> $BASH_ENV
            echo "export GH_TOKEN=$token" >> $BASH_ENV
            echo "export GITHUB_ACCESS_TOKEN=$token" >> $BASH_ENV

jobs:

  infrastructure:
    description: Build and optionally deploy the infratructure
    parameters:
      scope:
        description: The pnpm scope to build for
        type: string
      stack-output-path:
        description: The pnpm output path
        type: string
      apply:
        description: If you should apply
        type: boolean
        default: false
      dev:
        description: Whether or not its a dev build
        type: boolean
        default: false
      workspace:
        description: The terraform workspace
        type: string
      save_app_spec:
        description: Whether or not we should save off the app spec file for later use
        type: boolean
        default: true
      <<: [*repo_for_enum, *resource_class_enum]
    # Our self hosted runners dont support docker images, cause its not deployed in kubernetes, so we have some special steps
    machine: true
    resource_class: << parameters.resource-class >>
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - checkout
      - restore_cache:
          name: Restore Tfenv
          keys:
            - tfenv-v2
      - run:
          name: Install tfcmt
          # make tfcmt executable for all users
          command: |
            curl -L https://github.com/suzuki-shunsuke/tfcmt/releases/download/v4.7.3/tfcmt_linux_amd64.tar.gz | tar xvzf - tfcmt
            mv tfcmt /home/circleci/tfcmt
            chmod a+x /home/circleci/tfcmt
      - install_infrastructure_pnpm
      - install_codebuild_secrets
      - setup_github_bot
      - when:
          condition: <<parameters.dev>>
          steps:
            - run:
                name: Build Dev Infra
                command: |
                  . /home/circleci/.codebuild_shims_wrapper.sh
                  nvm use
                  export NODE_ENV=development
                  pnpm run --filter=<< parameters.scope >>... build
                  pnpm run --filter=<< parameters.scope >> synth
      - unless:
          condition: <<parameters.dev>>
          steps:
            - run:
                name: Build Prod Infra
                command: |
                  . /home/circleci/.codebuild_shims_wrapper.sh
                  nvm use
                  export NODE_ENV=production
                  pnpm run --filter=<< parameters.scope >>... build
                  pnpm run --filter=<< parameters.scope >> synth
      - run:
          name: Setup terraform
          command: |
            . /home/circleci/.codebuild_shims_wrapper.sh
            cd << parameters.stack-output-path >>
            tfenv use
            terraform init
      - when:
          condition: <<parameters.save_app_spec>>
          steps:
            - restore_cache:
                name: Restore AppSpec Cache
                keys:
                  - appspec-<< parameters.for >>
      - when:
          condition: <<parameters.apply>>
          steps:
            - attach_workspace:
                at: /tmp/workspace
            - run:
                name: Terraform apply
                command: |
                  . /home/circleci/.codebuild_shims_wrapper.sh
                  cd << parameters.stack-output-path >>
                  /home/circleci/tfcmt --var target:<< parameters.scope >><<#parameters.dev>>-dev<</parameters.dev>> apply -- terraform apply -auto-approve -lock-timeout=10m
            - when:
                condition: <<parameters.save_app_spec>>
                steps:
                  - run:
                      name: Copy App Spec
                      command: |
                        . /home/circleci/.codebuild_shims_wrapper.sh
                        cd << parameters.stack-output-path >>
                        cp appspec.json /tmp/workspace/
                  - persist_to_workspace:
                      root: /tmp/workspace
                      paths:
                        - 'appspec.json'
      - unless:
          condition: <<parameters.apply>>
          steps:
            - run:
                name: Terraform plan
                command: |
                  . /home/circleci/.codebuild_shims_wrapper.sh
                  cd << parameters.stack-output-path >>
                  /home/circleci/tfcmt --var target:<< parameters.scope >><<#parameters.dev>>-dev<</parameters.dev>> plan --skip-no-changes --patch -- terraform plan -lock-timeout=10m
      - save_cache:
          key: tfenv-v2
          paths:
            - /home/circleci/.tfenv/versions/*
            - /home/circleci/.tfenv/bin/terraform-*
      - when:
          condition: <<parameters.save_app_spec>>
          steps:
            - save_cache:
                key: appspec-<< parameters.for >>
                paths:
                  - << parameters.stack-output-path >>/appspec.json
  code_deploy_ecs:
    parameters:
      workspace:
        description: Workspace where the appspec.json are at
        type: string
        default: /tmp/workspace
      codedeploy-app-name:
        description: CodeDeploy app name
        type: string
      codedeploy-group-name:
        description: CodeDeploy group name
        type: string
      <<: [*repo_for_enum, *resource_class_enum]
    # Our self hosted runners dont support docker images, cause its not deployed in kubernetes, so we have some special steps
    machine: true
    resource_class: << parameters.resource-class >>
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - attach_workspace:
          at: << parameters.workspace >>
      - run:
          name: CodeDeploy
          command: |
            . /home/circleci/.codebuild_shims_wrapper.sh
            export AWS_PAGER=""
            currentDeployment=$(aws deploy list-deployments --application-name "<< parameters.codedeploy-app-name >>" --deployment-group-name "<< parameters.codedeploy-group-name >>" --query "deployments[?status=='InProgress'].deploymentId" --output text --no-cli-pager)
            if [ -n "$currentDeployment" ]; then
              echo "There is already a deployment in progress with ID: $currentDeployment" waiting for it to finish
              aws deploy wait deployment-successful --deployment-id "$deploymentId" --no-cli-pager || {
                echo "Deployment failed or timed out."
                exit 1
              }
            fi

            export appspec=$(cat '<< parameters.workspace >>/appspec.json')
            export REVISION="revisionType=AppSpecContent,appSpecContent={content='$appspec'}"
            aws deploy \
            create-deployment \
            --application-name="<< parameters.codedeploy-app-name >>"  \
            --deployment-group-name="<< parameters.codedeploy-group-name >>" \
            --description="Triggered from CircleCI" \
            --revision="$REVISION" \
            --no-cli-pager

  code_deploy_lambda:
    parameters:
      codedeploy-app-name:
        description: CodeDeploy app name
        type: string
      codedeploy-group-name:
        description: CodeDeploy group name
        type: string
      function-name:
        description: >
          The name of the Lambda Function to deploy to
        type: string
      s3-bucket:
        type: string
        description: The name of the bucket to deploy from
      s3-key:
        type: string
        description: The name of the s3 key that contains the code to deploy
        default: ""
      function-alias:
        type: string
        description: The name of the lambda alias to use
        default: DEPLOYED
      <<: [*repo_for_enum, *resource_class_enum]
    # Our self hosted runners dont support docker images, cause its not deployed in kubernetes, so we have some special steps
    machine: true
    resource_class: << parameters.resource-class >>
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - jq/install
      - run:
          name: Deploy Lambda
          command: |
            . /home/circleci/.codebuild_shims_wrapper.sh
            export AWS_PAGER=""
            aws lambda wait function-updated --function-name '<< parameters.function-name >>'

            s3Key="<< parameters.s3-key >>"
            if [[ -z $s3Key ]]; then
                s3Key="$CIRCLE_SHA1.zip"
            fi

            aws lambda update-function-code \
                --function-name '<< parameters.function-name >>' \
                --s3-bucket '<< parameters.s3-bucket >>' \
                --s3-key "$s3Key"

            aws lambda wait function-updated --function-name '<< parameters.function-name >>'

            versionId=$(aws lambda publish-version \
                --function-name '<< parameters.function-name >>' | jq -r .Version)

            currentVersion=$(aws lambda get-alias \
                --function-name '<< parameters.function-name >>' \
                --name DEPLOYED | jq -r .FunctionVersion)

            app_spec_content_string="{'version':0.0,'Resources':[{'<< parameters.function-name >>':{'Type':'AWS::Lambda::Function','Properties':{'Name':'<< parameters.function-name >>','Alias':'<< parameters.function-alias >>','TargetVersion':'$versionId', 'CurrentVersion': '$currentVersion'}}}]}"
            echo "$app_spec_content_string"
            app_spec_content_sha256=$(echo -n "$app_spec_content_string" | shasum -a 256 | sed 's/ .*$//')
            revision="revisionType=AppSpecContent,appSpecContent={content=\"$app_spec_content_string\",sha256=$app_spec_content_sha256}"

            aws lambda wait function-updated --function-name '<< parameters.function-name >>'

            aws deploy create-deployment \
              --application-name="<< parameters.codedeploy-app-name >>" \
              --deployment-group-name="<< parameters.codedeploy-group-name >>" \
              --description="Triggered build $CIRCLE_SHA1 from CircleCI" \
              --revision="$revision"

  test_integrations:
    description: Run integration tests against external services, e.g. MySQL
    parameters:
      run_prisma_jobs:
        type: boolean
        description: Indicates if prisma jobs needs to be run
        default: false
      scope:
        description: The pnpm scope to run tests for
        type: string
      <<: *repo_for_enum
    docker:
      - image: *node_image
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
        environment:
          AWS_XRAY_LOG_LEVEL: silent
          AWS_XRAY_CONTEXT_MISSING: LOG_ERROR
      - image: mysql:8.0
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
        environment:
          - MYSQL_ALLOW_EMPTY_PASSWORD=yes
          - TZ=UTC
        command: --default_authentication_plugin=mysql_native_password --sql-mode="NO_ENGINE_SUBSTITUTION" --character-set-server=UTF8MB4 --collation-server=utf8mb4_unicode_ci
      - image: localstack/localstack:3.0.2
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
        environment:
          SERVICES: s3,dynamodb
      - image: pocket/snowplow-micro:prod
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
    steps:
      - checkout
      - install_pnpm:
          scope: << parameters.scope >>
      - when:
          condition:
            equal: [ true, << parameters.run_prisma_jobs >> ]
          steps:
            - run:
                name: generate prisma client
                command: pnpm run --filter=<< parameters.scope >> db:generate-prisma-client
            - run:
                name: apply prisma migration
                command: |
                  export $(egrep -v '^#' .docker/local.env | xargs -0)
                  pnpm run --filter=<< parameters.scope >> migrate:prisma-reset
      - run:
          name: run setup.sh
          command: |
            export $(egrep -v '^#' .docker/local.env | xargs -0) && ./.circleci/scripts/setup.sh --db --aws
      - run:
          # Note there is a bug in turbo repo requiring a build https://github.com/vercel/turbo/issues/1609
          name: run tests
          command: |
            export $(egrep -v '^#' .docker/local.env | xargs -0)
            pnpm run --filter=<< parameters.scope >>... build
            pnpm run --filter=<< parameters.scope >>... test-integrations -- --watchAll=false --forceExit --detectOpenHandles

  build_image:
    description: Build and/or push docker image to ECR.

    parameters:
      run_prisma_jobs:
        type: boolean
        description: Indicates if prisma jobs needs to be run
        default: false
      aws-access-key-id:
        description: 'AWS access key id environment variable'
        type: string
      aws-region:
        description: 'AWS region value'
        type: string
      aws-secret-access-key:
        description: 'AWS secret access key environment variable'
        type: string
      ecr-url:
        description: 'The ecr url'
        type: string
      extra-build-args:
        description: 'Extra flags to pass to docker build. For examples, see https://docs.docker.com/engine/reference/commandline/build'
        type: string
        default: --build-arg GIT_SHA=${CIRCLE_SHA1}
      push:
        description: 'Whether or not to push the code'
        type: boolean
        default: false
      repo-name:
        description: 'The ecr repo name'
        type: string
      tag:
        description: 'The docker tag name'
        type: string
        default: latest,$CIRCLE_SHA1
      <<: *repo_for_enum
    executor: aws-cli/default

    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - checkout
      - aws-cli/setup:
          aws-access-key-id: << parameters.aws-access-key-id >>
          aws-secret-access-key: << parameters.aws-secret-access-key >>
          aws-region: << parameters.aws-region >>
      - run:
          name: Setup common environment variables
          command: |
            { \
              echo 'export AWS_ECR_ACCOUNT_URL="<< parameters.ecr-url >>"'; \
              echo 'export REPO_NAME="<< parameters.repo-name >>"'; \
            } >> "$BASH_ENV"
      - when:
          condition: <<parameters.push>>
          steps:
            - aws-ecr/build-and-push-image:
                checkout: false
                repo: << parameters.repo-name >>
                setup-remote-docker: true
                remote-docker-layer-caching: true
                aws-access-key-id: << parameters.aws-access-key-id >>
                aws-secret-access-key: << parameters.aws-secret-access-key >>
                tag: << parameters.tag >>
                remote-docker-version: default
                extra-build-args: << parameters.extra-build-args >>
      - unless:
          condition: <<parameters.push>>
          steps:
            - setup_remote_docker:
                version: default
                docker-layer-caching: true
            - aws-ecr/build-image:
                repo: << parameters.repo-name >>
                tag: << parameters.tag >>
                extra-build-args: << parameters.extra-build-args >>

  build_lambda:
    description: Build and/or push lambda function.
    parameters:
      aws-access-key-id:
        description: 'AWS access key id environment variable'
        type: string
      aws-region:
        description: 'AWS region value'
        type: string
      aws-secret-access-key:
        description: 'AWS secret access key environment variable'
        type: string
      s3-bucket:
        description: 'The s3 bucket name'
        type: string
        default: ""
      scope:
        description: The pnpm scope to build for
        type: string
      <<: *repo_for_enum
    docker:
      - image: *node_image
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - run:
          name: Setup Environment variables
          command: |
            echo "export SENTRY_AUTH_TOKEN="$SENTRY_BEARER"" >> "$BASH_ENV"
      - checkout
      - install_pnpm:
          scope: << parameters.scope >>
      - run:
          # Theres a really annoying bug in PNPM deploy command that will try and create a folder at /home/pruned which we are not allowed to do,
          # so we move it under 1 directory to let it do its thing.
          # https://github.com/pnpm/pnpm/issues/5086
          # We also go crazy on the deploy command per https://github.com/pnpm/pnpm/issues/6166#issuecomment-1802541463
          name: Build lambda
          command: |
            pnpm install --filter=<< parameters.scope >>... --frozen-lockfile
            pnpm run --filter=<< parameters.scope >>... build
            mkdir -p ~/bug/project
            cp -R . ~/bug/project/
            cd ~/bug/project/
            pnpm --config.shamefully-hoist=true --config.hoist=true --config.node-linker=true --config.symlinks=false --config.shared-workspace-lockfile=false deploy --filter=<< parameters.scope >> --prod pruned
      - run:
          name: Package Lambda
          command: |
            cd ~/bug/project/pruned
            cp -r package.json dist/
            cp -r node_modules/ dist/node_modules/

            cd dist
            zip -r9 ~/project/${CIRCLE_SHA1}.zip .
            mkdir /tmp/artifacts
            cp ~/project/${CIRCLE_SHA1}.zip /tmp/artifacts/
            cd ..
            maxFileSize=256000 # Get the size of the directory in kilobytes
            export dirSize=$(du -s dist | cut -f1) 
            echo "Size is: $dirSize"
            if ((dirSize > maxFileSize)); then
              echo "Directory size is equal to or larger than $maxFileSize KB. which is the lambda limit"
              exit 1
            fi
      - when:
          condition: << parameters.s3-bucket >>
          steps:
            - aws-cli/setup:
                aws-access-key-id: << parameters.aws-access-key-id >>
                aws-secret-access-key: << parameters.aws-secret-access-key >>
                aws-region: << parameters.aws-region >>
            - run:
                name: Upload Package
                command: aws s3 cp ${CIRCLE_SHA1}.zip s3://<< parameters.s3-bucket >>/
      - store_artifacts:
          path: /tmp/artifacts

  apollo:
    description: >
      Runs Apollo rover schema check on the production graphql federated schema.
      If it is the production branch will deploy the subgraph to the production federated graph.
      If the branch is the development branch, will deploy the subgraph to the development federated graph.

    parameters:
      multiple_schema:
        type: boolean
        description: if service has more than 1 graphql schema to deploy
        default: false
      fed_graph_name:
        type: string
        description: The name of federated graph to check
      client_graph_name:
        type: string
        description: The name of client graph to check
        default: ""
      graph_name:
        type: string
        description: The name of this subgraph
      schema_file_path:
        type: string
        description: The path to the schema file
        default: ./schema.graphql
      admin_schema_file_path:
        type: string
        description: The path to the admin schema file
        default: ./schema-admin.graphql
      shared_schema_file_path:
        type: string
        description: The path to the shared schema file
        default: ./schema-shared-admin.graphql
      prod_graph_url:
        type: string
        description: The production subgraph url
      dev_graph_url:
        type: string
        description: The development subgraph url
      prod_graph_variant_name:
        type: string
        description: The production variant graph name
        default: "current"
      dev_graph_variant_name:
        type: string
        description: The development variant graph name
        default: "development"
      prod_branch:
        type: string
        description: The production git branch
        default: "main"
      dev_branch:
        type: string
        description: The development git branch
        default: "dev"
      apollo_key_env:
        type: env_var_name
        default: APOLLO_KEY
        description: The environment variable name of the apollo key to user
      build_command:
        description: 'build command to use if we need to'
        type: string
        default: ""
      scope:
        description: The pnpm recognized name of the service. Used to scope the packages installed to only that service.
        type: string

    docker:
      - image: *node_image
        auth:
          username: $DOCKER_LOGIN
          password: $DOCKER_PASSWORD

    steps:
      - checkout
      - run:
          name: install rover
          command: |
            # download and install Rover
            curl -sSL https://rover.apollo.dev/nix/latest | sh

            # This allows the PATH changes to persist to the next `run` step
            echo "export PATH=$HOME/.rover/bin:$PATH" >> "$BASH_ENV"
      - when:
          condition: << parameters.build_command >>
          steps:
            - install_pnpm:
                scope: << parameters.scope >>
            - run:
                name: build schema
                command: |
                  << parameters.build_command >>
      - when:
          condition:
            equal: [ true, << parameters.multiple_schema >> ]
          steps:
            - run:
                name: build admin schema
                command: cat << parameters.shared_schema_file_path >> << parameters.admin_schema_file_path >> > schema-admin-api.graphql
            - run:
                name: build public schema
                command: cat << parameters.shared_schema_file_path >> << parameters.schema_file_path >> > schema.graphql
            - run:
                name: check client schema
                command: rover subgraph check << parameters.client_graph_name >>@<< parameters.prod_graph_variant_name >> --schema ./schema.graphql --name=<< parameters.graph_name >>
            - run:
                name: check admin schema
                command: |
                  export APOLLO_KEY=$<< parameters.apollo_key_env >>
                  rover subgraph check << parameters.fed_graph_name >>@<< parameters.prod_graph_variant_name >> --schema ./schema-admin-api.graphql --name=<< parameters.graph_name >>
      - when:
          condition:
            equal: [ false, << parameters.multiple_schema >> ]
          steps:
            - run:
                name: check service
                command: |
                  export APOLLO_KEY=$<< parameters.apollo_key_env >>
                  rover subgraph check << parameters.fed_graph_name >>@<< parameters.prod_graph_variant_name >> --schema << parameters.schema_file_path >> --name=<< parameters.graph_name >>
      - when:
          condition:
            and:
              - equal: [ << parameters.prod_branch >>, << pipeline.git.branch >> ]
              - equal: [ true, << parameters.multiple_schema >> ]
          steps:
            - run:
                name: push service to prod client api
                command: rover subgraph publish << parameters.client_graph_name >>@<< parameters.prod_graph_variant_name >> --schema ./schema.graphql --routing-url << parameters.prod_graph_url >> --name=<< parameters.graph_name >>
            - run:
                name: push service to prod admin api
                command: |
                  export APOLLO_KEY=$ADMIN_APOLLO_KEY
                  rover subgraph publish << parameters.fed_graph_name >>@<< parameters.prod_graph_variant_name >> --schema ./schema-admin-api.graphql --routing-url << parameters.prod_graph_url >>/admin --name=<< parameters.graph_name >>
      - when:
          condition:
            and:
              - equal: [ << parameters.dev_branch >>, << pipeline.git.branch >> ]
              - equal: [ true, << parameters.multiple_schema >> ]
          steps:
            - run:
                name: push service to dev client api
                command: rover subgraph publish << parameters.client_graph_name >>@<< parameters.dev_graph_variant_name >> --schema ./schema.graphql --routing-url << parameters.dev_graph_url >> --name=<< parameters.graph_name >>
            - run:
                name: push service to dev admin api
                command: |
                  export APOLLO_KEY=$ADMIN_APOLLO_KEY
                  rover subgraph publish << parameters.fed_graph_name >>@<< parameters.dev_graph_variant_name >> --schema ./schema-admin-api.graphql --routing-url << parameters.dev_graph_url >>/admin --name=<< parameters.graph_name >>
      - when:
          condition:
            and:
              - equal: [<< parameters.prod_branch >>, << pipeline.git.branch >>]
              - equal: [ false, << parameters.multiple_schema >> ]
          steps:
            - run:
                name: push service to prod
                command: |
                  export APOLLO_KEY=$<< parameters.apollo_key_env >>
                  rover subgraph publish << parameters.fed_graph_name >>@<< parameters.prod_graph_variant_name >> --schema << parameters.schema_file_path >> --routing-url << parameters.prod_graph_url >> --name=<< parameters.graph_name >>
      - when:
          condition:
            and:
              - equal: [<< parameters.dev_branch >>, << pipeline.git.branch >>]
              - equal: [ false, << parameters.multiple_schema >> ]
          steps:
            - run:
                name: push service to dev
                command: |
                  export APOLLO_KEY=$<< parameters.apollo_key_env >>
                  rover subgraph publish << parameters.fed_graph_name >>@<< parameters.dev_graph_variant_name >> --schema << parameters.schema_file_path >> --routing-url << parameters.dev_graph_url >> --name=<< parameters.graph_name >>

  sentry_release_notification:
    description: Create new release in Sentry
    resource_class: small
    parameters:
      sentry_project_name:
        type: string
        description: the Sentry project name
      sentry_env:
        type: string
        default: Prod
        description: Which environment the release is going to
      sentry_org:
        type: string
        description: The sentry org
      <<: *repo_for_enum
    docker:
      - image: getsentry/sentry-cli
        auth:
          username: $DOCKER_LOGIN
          password: $DOCKER_PASSWORD
    steps:
      - exit-early-if-irrelevant:
          for: << parameters.for >>
      - run:
          name: Setup Environment variables
          command: |
            echo "export SENTRY_AUTH_TOKEN="$SENTRY_BEARER"" >> "$BASH_ENV"
            echo "export SENTRY_ORG=<< parameters.sentry_org >>" >> "$BASH_ENV"
            echo "export SENTRY_PROJECT=<< parameters.sentry_project_name >>" >> "$BASH_ENV"
      - run:
          name: Sentry Release Notification
          command: |
            source "$BASH_ENV"
            sentry-cli releases new "$CIRCLE_SHA1"
            sentry-cli releases set-commits "$CIRCLE_SHA1" --commit "Pocket/content-monorepo@$CIRCLE_SHA1"
            sentry-cli releases finalize "$CIRCLE_SHA1"
      - run:
          name: Sentry Deploy Notification
          command: |
            source "$BASH_ENV"
            sentry-cli releases deploys "$CIRCLE_SHA1" new -e "<< parameters.sentry_env >>"

  setup_deploy_params:
    description: Sets up the CircleCI variables in AWS using the service name and env that is passed
    parameters:
      service_name:
        description: Service Name
        type: string
      env:
        description: Environment of the service
        type: string
      aws_access_key_id:
        type: env_var_name
        default: AWS_ACCESS_KEY_ID
        description: AWS access key id for IAM role
      aws_secret_access_key:
        type: env_var_name
        default: AWS_SECRET_ACCESS_KEY
        description: AWS secret key for IAM role
    docker:
      - image: amazon/aws-cli:latest
        auth:
          username: $DOCKER_LOGIN
          password: $DOCKER_PASSWORD
        environment:
          TERM: xterm

    steps:
      - run:
          name: Put SSM Parameters
          command: |
            export AWS_ACCESS_KEY_ID="${<< parameters.aws_access_key_id >>}"
            export AWS_SECRET_ACCESS_KEY="${<< parameters.aws_secret_access_key >>}"
            aws ssm put-parameter --name "/<< parameters.service_name >>/CircleCI/<< parameters.env >>/BUILD_BRANCH" --type "SecureString" --value "${CIRCLE_BRANCH}" --overwrite
            aws ssm put-parameter --name "/<< parameters.service_name >>/CircleCI/<< parameters.env >>/SERVICE_VERSION" --type "SecureString" --value "${CIRCLE_BUILD_NUM}" --overwrite
            aws ssm put-parameter --name "/<< parameters.service_name >>/CircleCI/<< parameters.env >>/SERVICE_HASH" --type "SecureString" --value "${CIRCLE_SHA1}" --overwrite
